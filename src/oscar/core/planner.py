"""
OSCAR LLM Planner - Streamlined planning engine

Role: Converts natural language requests into structured action plans.

What it does:
- Sends the request to an LLM (Groq or Gemini)
- Gets back a structured JSON plan with step-by-step actions
- Assesses risk levels for each step
"""

import json
import re
import time
import platform
from typing import List, Dict, Any
from pathlib import Path
from pydantic import BaseModel, ValidationError
from rich.console import Console

from oscar.config.settings import settings, SAFETY_PATTERNS

console = Console()

class ActionStep(BaseModel):
    """Individual action step in a plan."""
    id: int
    tool: str
    command: str
    explanation: str
    risk_level: str = "low"


class AgentPlan(BaseModel):
    """Complete plan generated by the LLM."""
    thoughts: str
    plan: List[ActionStep]
    risk_level: str = "low"
    confirm_prompt: str


class LLMPlanner:
    """Streamlined LLM planning engine."""
    
    def __init__(self):
        self.config = settings.get_active_llm_config()
        self.provider = settings.llm_config.active_provider
        
        # Initialize LLM client
        self.client = self._init_client()
        
        # System context (only what we actually use)
        self.system_context = {
            "os_type": platform.system(),
            "cwd": str(Path.cwd()),
            "available_tools": ["shell", "web_search", "file_ops"]
        }
    
    def _init_client(self):
        """Initialize the LLM client based on provider."""
        try:
            api_key = settings.get_api_key(self.provider)
            
            if self.provider == "groq":
                from groq import Groq
                return Groq(api_key=api_key)
            elif self.provider == "gemini":
                import google.genai as genai
                genai.configure(api_key=api_key)
                return genai
            elif self.provider == "openai":
                from openai import OpenAI
                return OpenAI(api_key=api_key)
            else:
                raise ValueError(f"Unsupported LLM provider: {self.provider}")
                
        except Exception as e:
            raise RuntimeError(f"Failed to initialize LLM client: {e}")
    
    def create_plan(self, user_input: str, context: str = "") -> AgentPlan:
        """Create a structured plan from natural language input."""
        timings = {}
        total_start = time.time()
        
        # Build prompts
        t0 = time.time()
        system_prompt = self._build_system_prompt()
        user_prompt = self._build_user_prompt(user_input, context)
        timings['prompt_build'] = time.time() - t0
        
        # Call LLM
        t0 = time.time()
        raw_response = self._call_llm(system_prompt, user_prompt)
        timings['llm_call'] = time.time() - t0
        
        # Parse and validate response
        t0 = time.time()
        plan = self._parse_llm_response(raw_response)
        timings['parse'] = time.time() - t0
        
        # Assess overall risk
        plan.risk_level = self._assess_overall_risk(plan)
        
        timings['total'] = time.time() - total_start
        
        # Log timing breakdown
        console.print(f"[dim]â± Timing: LLM={timings['llm_call']:.1f}s, Parse={timings['parse']:.2f}s, Total={timings['total']:.1f}s[/dim]")
        
        return plan
    
    def _build_system_prompt(self) -> str:
        """Build the system prompt with context."""
        base_prompt = settings.llm_config.system_prompt
        
        # Format with current context
        formatted_prompt = base_prompt.format(
            os_type=self.system_context["os_type"],
            cwd=self.system_context["cwd"]
        )
        
        context_info = f"""
            Current System Context:
            - Operating System: {self.system_context['os_type']}
            - Current Directory: {self.system_context['cwd']}
            - Available Tools: {', '.join(self.system_context['available_tools'])}
            - Safety Mode: {'ON' if settings.safe_mode else 'OFF'}
            - Dry Run Mode: {'ON' if settings.dry_run_mode else 'OFF'}

            CRITICAL: You must respond with ONLY valid JSON in this exact format:
            {{
                "thoughts": "Your reasoning about the user's request",
                "plan": [
                    {{
                        "id": 1,
                        "tool": "shell|browser|file_ops",
                        "command": "exact command to execute directly in terminal with NO errors",
                        "explanation": "what this step does",
                        "risk_level": "low|medium|high|dangerous"
                    }}
                ],
                "risk_level": "low|medium|high|dangerous",
                "confirm_prompt": "Clear question asking user to approve the plan"
            }}
            """
        
        return formatted_prompt + context_info
    
    def _build_user_prompt(self, user_input: str, context: str) -> str:
        """Build the user prompt."""
        return settings.llm_config.planning_template.format(
            user_input=user_input,
            context=context if context else "No previous context"
        )
    
    def _call_llm(self, system_prompt: str, user_prompt: str) -> str:
        """Call the LLM API based on provider."""
        try:
            if self.provider == "gemini":
                # Gemini uses different API structure
                model = self.client.GenerativeModel(self.config.model)
                combined_prompt = f"{system_prompt}\n\n{user_prompt}"
                response = model.generate_content(
                    combined_prompt,
                    generation_config={
                        "max_output_tokens": self.config.max_tokens,
                        "temperature": self.config.temperature
                    }
                )
                return response.text
            else:
                # Groq/OpenAI use OpenAI-compatible API
                messages = [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ]
                
                response = self.client.chat.completions.create(
                    model=self.config.model,
                    messages=messages,
                    max_tokens=self.config.max_tokens,
                    temperature=self.config.temperature
                )
                return response.choices[0].message.content
            
        except Exception as e:
            raise RuntimeError(f"LLM API call failed: {e}")
    
    def _parse_llm_response(self, raw_response: str) -> AgentPlan:
        """Parse and validate the LLM response."""
        try:
            # Debug: show first 500 chars of raw response
            console.print(f"[dim] Raw response (first 500 chars): {raw_response[:500]}...[/dim]")
            
            # Clean the response
            cleaned_response = self._clean_json_response(raw_response)
            
            # Debug: show cleaned response
            console.print(f"[dim] Cleaned (first 300 chars): {cleaned_response[:300]}...[/dim]")
            
            # Parse JSON
            response_data = json.loads(cleaned_response)
            
            # Validate with Pydantic
            return AgentPlan(**response_data)
            
        except json.JSONDecodeError as e:
            console.print(f"[red]JSON Error: {e}[/red]")
            console.print(f"[red]Response was: {cleaned_response[:200]}[/red]")
            raise ValueError(f"Invalid JSON response from LLM: {e}")
        except ValidationError as e:
            console.print(f"[red]Validation Error: {e}[/red]")
            raise ValueError(f"Invalid plan structure: {e}")
    
    def _clean_json_response(self, response: str) -> str:
        """Clean LLM response to extract valid JSON.
        
        Handles qwen/qwen3-32b which outputs reasoning/thinking before JSON.
        """
        # Remove markdown code blocks
        response = re.sub(r'```json\s*', '', response)
        response = re.sub(r'```\s*', '', response)
        
        # Try to find a complete JSON object with nested structure
        # This regex handles nested braces properly
        brace_count = 0
        start_idx = -1
        end_idx = -1
        
        for i, char in enumerate(response):
            if char == '{':
                if brace_count == 0:
                    start_idx = i
                brace_count += 1
            elif char == '}':
                brace_count -= 1
                if brace_count == 0 and start_idx != -1:
                    end_idx = i
                    break
        
        if start_idx != -1 and end_idx != -1:
            extracted = response[start_idx:end_idx + 1]
            # Validate it's actually JSON-like
            if '"thoughts"' in extracted or '"plan"' in extracted:
                return extracted
        
        # Fallback: simple extraction
        start_idx = response.find('{')
        end_idx = response.rfind('}')
        
        if start_idx != -1 and end_idx != -1:
            return response[start_idx:end_idx + 1]
        
        return response.strip()
    
    def _assess_overall_risk(self, plan: AgentPlan) -> str:
        """Assess overall risk level for the plan."""
        risk_levels = ["low", "medium", "high", "dangerous"]
        max_risk = "low"
        
        for step in plan.plan:
            # Update step risk based on command analysis
            step.risk_level = self._assess_step_risk(step.command)
            
            # Track highest risk level
            if risk_levels.index(step.risk_level) > risk_levels.index(max_risk):
                max_risk = step.risk_level
        
        return max_risk
    
    def _assess_step_risk(self, command: str) -> str:
        """Assess the risk level of a single command."""
        command_lower = command.lower()
        
        # Check for dangerous patterns
        for pattern in SAFETY_PATTERNS["dangerous_commands"]:
            if re.search(pattern, command, re.IGNORECASE):
                return "dangerous"
        
        # Check for high-risk keywords
        for keyword in SAFETY_PATTERNS["high_risk_keywords"]:
            if keyword.lower() in command_lower:
                return "high"
        
        # Check for medium-risk keywords
        for keyword in SAFETY_PATTERNS["medium_risk_keywords"]:
            if keyword.lower() in command_lower:
                return "medium"
        
        return "low"
    
    def test_connection(self) -> Dict[str, Any]:
        """Test the LLM connection."""
        try:
            test_response = self._call_llm(
                "You are a test assistant. Respond with: {'status': 'ok', 'message': 'connection successful'}",
                "Test connection"
            )
            
            return {
                "status": "success",
                "provider": self.provider,
                "model": self.config.model,
                "response": test_response.strip()
            }
            
        except Exception as e:
            return {
                "status": "error",
                "provider": self.provider,
                "model": self.config.model,
                "error": str(e)
            }