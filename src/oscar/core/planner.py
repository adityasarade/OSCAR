"""
OSCAR LLM Planner - Streamlined planning engine
Converts natural language requests into structured action plans.
"""

import json
import re
import platform
from typing import List, Dict, Any
from pathlib import Path
from groq import Groq
from pydantic import BaseModel, ValidationError

from oscar.config.settings import settings, SAFETY_PATTERNS


class ActionStep(BaseModel):
    """Individual action step in a plan."""
    id: int
    tool: str
    command: str
    explanation: str
    risk_level: str = "low"


class AgentPlan(BaseModel):
    """Complete plan generated by the LLM."""
    thoughts: str
    plan: List[ActionStep]
    risk_level: str = "low"
    confirm_prompt: str


class LLMPlanner:
    """Streamlined LLM planning engine."""
    
    def __init__(self):
        self.config = settings.get_active_llm_config()
        self.provider = settings.llm_config.active_provider
        
        # Initialize LLM client
        self.client = self._init_client()
        
        # System context (only what we actually use)
        self.system_context = {
            "os_type": platform.system(),
            "cwd": str(Path.cwd()),
            "available_tools": ["shell", "browser", "file_ops"]
        }
    
    def _init_client(self):
        """Initialize the LLM client."""
        try:
            api_key = settings.get_api_key(self.provider)
            
            if self.provider == "groq":
                return Groq(api_key=api_key)
            elif self.provider == "openai":
                from openai import OpenAI
                return OpenAI(api_key=api_key)
            else:
                raise ValueError(f"Unsupported LLM provider: {self.provider}")
                
        except Exception as e:
            raise RuntimeError(f"Failed to initialize LLM client: {e}")
    
    def create_plan(self, user_input: str, context: str = "") -> AgentPlan:
        """Create a structured plan from natural language input."""
        
        # Build prompts
        system_prompt = self._build_system_prompt()
        user_prompt = self._build_user_prompt(user_input, context)
        
        # Call LLM
        raw_response = self._call_llm(system_prompt, user_prompt)
        
        # Parse and validate response
        plan = self._parse_llm_response(raw_response)
        
        # Assess overall risk
        plan.risk_level = self._assess_overall_risk(plan)
        
        return plan
    
    def _build_system_prompt(self) -> str:
        """Build the system prompt with context."""
        base_prompt = settings.llm_config.system_prompt
        
        # Format with current context
        formatted_prompt = base_prompt.format(
            os_type=self.system_context["os_type"],
            cwd=self.system_context["cwd"]
        )
        
        context_info = f"""
Current System Context:
- Operating System: {self.system_context['os_type']}
- Current Directory: {self.system_context['cwd']}
- Available Tools: {', '.join(self.system_context['available_tools'])}
- Safety Mode: {'ON' if settings.safe_mode else 'OFF'}
- Dry Run Mode: {'ON' if settings.dry_run_mode else 'OFF'}

CRITICAL: You must respond with ONLY valid JSON in this exact format:
{{
    "thoughts": "Your reasoning about the user's request",
    "plan": [
        {{
            "id": 1,
            "tool": "shell|browser|file_ops",
            "command": "exact command to execute",
            "explanation": "what this step does",
            "risk_level": "low|medium|high|dangerous"
        }}
    ],
    "risk_level": "low|medium|high|dangerous",
    "confirm_prompt": "Clear question asking user to approve the plan"
}}
"""
        
        return formatted_prompt + context_info
    
    def _build_user_prompt(self, user_input: str, context: str) -> str:
        """Build the user prompt."""
        return settings.llm_config.planning_template.format(
            user_input=user_input,
            context=context if context else "No previous context"
        )
    
    def _call_llm(self, system_prompt: str, user_prompt: str) -> str:
        """Call the LLM API."""
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        try:
            if self.provider == "groq":
                response = self.client.chat.completions.create(
                    model=self.config.model,
                    messages=messages,
                    max_tokens=self.config.max_tokens,
                    temperature=self.config.temperature,
                    reasoning_effort="medium"  # For GPT-OSS reasoning
                )
            else:  # OpenAI
                response = self.client.chat.completions.create(
                    model=self.config.model,
                    messages=messages,
                    max_tokens=self.config.max_tokens,
                    temperature=self.config.temperature
                )
            
            return response.choices[0].message.content
            
        except Exception as e:
            raise RuntimeError(f"LLM API call failed: {e}")
    
    def _parse_llm_response(self, raw_response: str) -> AgentPlan:
        """Parse and validate the LLM response."""
        try:
            # Clean the response
            cleaned_response = self._clean_json_response(raw_response)
            
            # Parse JSON
            response_data = json.loads(cleaned_response)
            
            # Validate with Pydantic
            return AgentPlan(**response_data)
            
        except json.JSONDecodeError as e:
            raise ValueError(f"Invalid JSON response from LLM: {e}")
        except ValidationError as e:
            raise ValueError(f"Invalid plan structure: {e}")
    
    def _clean_json_response(self, response: str) -> str:
        """Clean LLM response to extract valid JSON."""
        # Remove markdown code blocks
        response = re.sub(r'```json\s*', '', response)
        response = re.sub(r'```\s*', '', response)
        
        # Extract JSON object
        start_idx = response.find('{')
        end_idx = response.rfind('}')
        
        if start_idx != -1 and end_idx != -1:
            response = response[start_idx:end_idx + 1]
        
        return response.strip()
    
    def _assess_overall_risk(self, plan: AgentPlan) -> str:
        """Assess overall risk level for the plan."""
        risk_levels = ["low", "medium", "high", "dangerous"]
        max_risk = "low"
        
        for step in plan.plan:
            # Update step risk based on command analysis
            step.risk_level = self._assess_step_risk(step.command)
            
            # Track highest risk level
            if risk_levels.index(step.risk_level) > risk_levels.index(max_risk):
                max_risk = step.risk_level
        
        return max_risk
    
    def _assess_step_risk(self, command: str) -> str:
        """Assess the risk level of a single command."""
        command_lower = command.lower()
        
        # Check for dangerous patterns
        for pattern in SAFETY_PATTERNS["dangerous_commands"]:
            if re.search(pattern, command, re.IGNORECASE):
                return "dangerous"
        
        # Check for high-risk keywords
        for keyword in SAFETY_PATTERNS["high_risk_keywords"]:
            if keyword.lower() in command_lower:
                return "high"
        
        # Check for medium-risk keywords
        for keyword in SAFETY_PATTERNS["medium_risk_keywords"]:
            if keyword.lower() in command_lower:
                return "medium"
        
        return "low"
    
    def test_connection(self) -> Dict[str, Any]:
        """Test the LLM connection."""
        try:
            test_response = self._call_llm(
                "You are a test assistant. Respond with: {'status': 'ok', 'message': 'connection successful'}",
                "Test connection"
            )
            
            return {
                "status": "success",
                "provider": self.provider,
                "model": self.config.model,
                "response": test_response.strip()
            }
            
        except Exception as e:
            return {
                "status": "error",
                "provider": self.provider,
                "model": self.config.model,
                "error": str(e)
            }